{"status":"ok","feed":{"url":"https://medium.com/feed/@@darshan.mohanty14","title":"Stories by Priyadarshan Mohanty on Medium","link":"https://medium.com/@darshan.mohanty14?source=rss-a0040ce85d9d------2","author":"","description":"Stories by Priyadarshan Mohanty on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*p5mBSzMS-KB4E6F77Kvc-w@2x.jpeg"},"items":[{"title":"Automating Key Pair Rotation in Snowflake using Airflow","pubDate":"2023-10-27 12:56:59","link":"https://medium.com/@darshan.mohanty14/automating-key-pair-rotation-in-snowflake-using-airflow-4557f998b6bf?source=rss-a0040ce85d9d------2","guid":"https://medium.com/p/4557f998b6bf","author":"Priyadarshan Mohanty","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*92jrYR1if3yYCi9i8nwWxg.jpeg","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*92jrYR1if3yYCi9i8nwWxg.jpeg\"></figure><p>Security is paramount in today\u2019s digital landscape, and Snowflake stands out as a leading cloud data platform for data warehousing and analytics. Snowflake offers robust security features, particularly concerning access control and authentication. Users often employ RSA key pairs for secure authentication. This article explores how to automate the process of rotating RSA key pairs for Snowflake users using Apache\u00a0Airflow.</p>\n<h3>Key Pair Authentication &amp;\u00a0Rotation</h3>\n<p>Before delving into the automation process, it\u2019s crucial to grasp the significance of key pair authentication and its rotation in Snowflake.</p>\n<p><strong>An Overview<br></strong>Snowflake introduces key pair authentication as a security measure, an alternative to the conventional basic authentication (i.e., username and password). This authentication method mandates at least a 2048-bit RSA key pair. You can generate this Privacy Enhanced Mail (PEM) private-public key pair using OpenSSL. Some Snowflake clients support encrypted private keys for connecting to Snowflake. The public key is associated with the Snowflake user who employs it for connecting and authenticating to Snowflake.</p>\n<p>Moreover, Snowflake also supports the rotation of public keys, a measure that enhances security and governance.</p>\n<p><strong>Configuring the Apache Airflow DAG<br></strong>In this guide, we will create an Apache Airflow Directed Acyclic Graph (DAG) that automates the rotation of RSA key pairs for Snowflake users. The following code snippet outlines the structure of the DAG responsible for this\u00a0task:</p>\n<pre># Import necessary libraries<br>import logging as log<br>from airflow import DAG<br>from airflow.exceptions import AirflowException<br>from airflow.operators.python import PythonOperator<br>from airflow.utils.dates import days_ago<br>from airflow.utils.email import send_email<br>from cryptography.hazmat.backends import default_backend<br>from cryptography.hazmat.primitives import serialization<br>from cryptography.hazmat.primitives.asymmetric import rsa<br><br># Define the DAG<br>default_args = {<br> \"owner\": \"airflow\",<br> \"start_date\": days_ago(1),<br> \"retries\": 1,<br>}<br><br>dag = DAG(<br> \"Snowflake_KeyPair_GEN\",<br> default_args=default_args,<br> schedule_interval=None, # Designed for ad-hoc execution only!<br> tags=[\"Snowflake\", \"KeyPair\", \"PWD\", \"RSA\"],<br>)<br># Define DAG documentation<br>dag.doc_md = \"\"\" This DAG is meant for ad-hoc execution and should be triggered via a DAG Run Config. Example:<br>{<br> \"snowflake_user\":\"xxxxx\",<br> \"email_id\" : \"abc@xyz.com\"<br>},<br>The DAG sends an email to the specified email_id, containing the new RSA keys attached, and BrandsBI in BCC.<br>\"\"\"<br># \u2026 (Continuing with the code)</pre>\n<p>In this code snippet, we import the necessary libraries, define the DAG with its default settings, and provide documentation on its usage. The DAG is intended for ad-hoc execution, triggered via a DAG Run Config that supplies the Snowflake user and email ID as parameters.</p>\n<p><strong>Generating RSA Key\u00a0Pairs</strong></p>\n<p>The core of the automation lies in the Python function responsible for generating RSA key pairs. Here\u2019s an excerpt from the code that performs this\u00a0task:</p>\n<pre># ... (Previous code)<br><br>def generate_rsa_keys(**kwargs):<br>    # Extract the Snowflake user and email ID from the DAG Run Config<br>    snowflake_user = kwargs[\"dag_run\"].conf.get(\"snowflake_user\").strip()<br>    email_id = kwargs[\"dag_run\"].conf.get(\"email_id\").strip()<br><br>    if snowflake_user is None or email_id is None:<br>        raise AirflowException(\"The snowflake_user or email_id is not provided\")<br><br>    # Generate a new private key and passphrase<br>    private_key = rsa.generate_private_key(<br>        public_exponent=65537, key_size=2048, backend=default_backend()<br>    )<br><br>    # Generate a random passphrase<br>    passphrase = generate_passphrase()<br><br>    # Serialize the private key into PEM format<br>    private_key_pem = private_key.private_bytes(<br>        encoding=serialization.Encoding.PEM,<br>        format=serialization.PrivateFormat.PKCS8,<br>        encryption_algorithm=serialization.BestAvailableEncryption(passphrase),<br>    )<br><br>    # Retrieve the corresponding public key<br>    public_key = private_key.public_key()<br><br>    # Serialize the public key into PEM format without delimiters<br>    public_key_pem_without_delimiters = (<br>        public_key.public_bytes(<br>            encoding=serialization.Encoding.PEM,<br>            format=serialization.PublicFormat.SubjectPublicKeyInfo,<br>        )<br>        .decode(\"utf-8\")<br>        .split(\"\\n\")[1:-2]<br>    )  # Excluding the first and last lines<br><br>    # Convert the public key into a string without delimiters<br>    public_key_string = \"\".join(public_key_pem_without_delimiters)<br><br>    # Modify the Snowflake user with the new public key<br>    operation_result = alter_sf_user(public_key_string, snowflake_user)<br><br>    if operation_result is False:<br>        raise AirflowException(\"Failed to alter the user\")<br><br>    # Send an email with the new RSA keys<br>    html_content = f\"\"\"<br>    &lt;!DOCTYPE html&gt;<br>    &lt;!-- (Email content) --&gt;<br>    \"\"\"<br>    # ... (Email sending)<br><br># ... (Continuing with the code)</pre>\n<pre>def generate_passphrase(length=16):<br>    import random<br>    import string<br><br>    # Define character sets for each category<br>    uppercase_letters = string.ascii_uppercase<br>    lowercase_letters = string.ascii_lowercase<br>    digits = string.digits<br>    special_characters = \"!@#$%^&amp;*()_-+=&lt;&gt;?/\\\\\"<br><br>    # Ensure that the passphrase meets the minimum length requirement<br>    if length &lt; 12:<br>        raise ValueError(\"Passphrase length must be at least 12 characters.\")<br><br>    # Create a list of characters to choose from<br>    character_set = uppercase_letters + lowercase_letters + digits + special_characters<br><br>    # Ensure at least one character from each category<br>    passphrase = (<br>        random.choice(uppercase_letters)<br>        + random.choice(lowercase_letters)<br>        + random.choice(digits)<br>        + random.choice(special_characters)<br>    )<br><br>    # Generate the remaining characters in the passphrase<br>    for _ in range(length - 4):<br>        passphrase += random.choice(character_set)<br><br>    # Shuffle the characters in the passphrase<br>    passphrase_list = list(passphrase)<br>    random.shuffle(passphrase_list)<br>    passphrase = \"\".join(passphrase_list)<br><br>    return passphrase.encode(\"utf-8\")<br><br>def alter_sf_user(public_key_string: str, snowflake_user: str) -&gt; bool:<br>    from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook<br><br>    snow_hook = SnowflakeHook(<br>        \"snowflake_conn\",<br>    )<br><br>    op = snow_hook.run(<br>        f\"ALTER USER {snowflake_user} SET RSA_PUBLIC_KEY = '{public_key_string}';\"<br>    )<br><br>    if op is not None:<br>        return False<br>    return True</pre>\n<p>In this function, we generate a new private key, extract the Snowflake user and email ID from the DAG Run Config, and modify the Snowflake user by setting the new public key. Finally, we send an email to the specified email ID, including the new RSA keys and passphrase.</p>\n<p><strong>In Conclusion</strong></p>\n<p>Automating the rotation of RSA key pairs for Snowflake users using Apache Airflow simplifies the security process and ensures that keys are frequently updated. This approach reduces the risks associated with prolonged key usage and enhances the overall security of your Snowflake data warehouse.</p>\n<p>By following the steps outlined in this guide and adapting the code to your specific Snowflake configuration, you can implement an effective and secure key pair rotation process in your organization. This approach not only enhances security but also minimizes the operational overhead typically associated with manual key management.</p>\n<p>Remember to schedule this Airflow DAG as needed or trigger it manually to ensure that your Snowflake users continually possess up-to-date and secure key pairs for authentication.</p>\n<p><strong>Useful Links</strong></p>\n<ul>\n<li><a href=\"https://docs.snowflake.com/en/user-guide/key-pair-auth\">https://docs.snowflake.com/en/user-guide/key-pair-auth</a></li>\n<li><a href=\"https://community.snowflake.com/s/article/How-To-Connect-to-Snowflake-using-key-pair-authentication-directly-using-the-private-key-incode-with-the-Python-Connector\">https://community.snowflake.com/s/article/How-To-Connect-to-Snowflake-using-key-pair-authentication-directly-using-the-private-key-incode-with-the-Python-Connector</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4557f998b6bf\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*92jrYR1if3yYCi9i8nwWxg.jpeg\"></figure><p>Security is paramount in today\u2019s digital landscape, and Snowflake stands out as a leading cloud data platform for data warehousing and analytics. Snowflake offers robust security features, particularly concerning access control and authentication. Users often employ RSA key pairs for secure authentication. This article explores how to automate the process of rotating RSA key pairs for Snowflake users using Apache\u00a0Airflow.</p>\n<h3>Key Pair Authentication &amp;\u00a0Rotation</h3>\n<p>Before delving into the automation process, it\u2019s crucial to grasp the significance of key pair authentication and its rotation in Snowflake.</p>\n<p><strong>An Overview<br></strong>Snowflake introduces key pair authentication as a security measure, an alternative to the conventional basic authentication (i.e., username and password). This authentication method mandates at least a 2048-bit RSA key pair. You can generate this Privacy Enhanced Mail (PEM) private-public key pair using OpenSSL. Some Snowflake clients support encrypted private keys for connecting to Snowflake. The public key is associated with the Snowflake user who employs it for connecting and authenticating to Snowflake.</p>\n<p>Moreover, Snowflake also supports the rotation of public keys, a measure that enhances security and governance.</p>\n<p><strong>Configuring the Apache Airflow DAG<br></strong>In this guide, we will create an Apache Airflow Directed Acyclic Graph (DAG) that automates the rotation of RSA key pairs for Snowflake users. The following code snippet outlines the structure of the DAG responsible for this\u00a0task:</p>\n<pre># Import necessary libraries<br>import logging as log<br>from airflow import DAG<br>from airflow.exceptions import AirflowException<br>from airflow.operators.python import PythonOperator<br>from airflow.utils.dates import days_ago<br>from airflow.utils.email import send_email<br>from cryptography.hazmat.backends import default_backend<br>from cryptography.hazmat.primitives import serialization<br>from cryptography.hazmat.primitives.asymmetric import rsa<br><br># Define the DAG<br>default_args = {<br> \"owner\": \"airflow\",<br> \"start_date\": days_ago(1),<br> \"retries\": 1,<br>}<br><br>dag = DAG(<br> \"Snowflake_KeyPair_GEN\",<br> default_args=default_args,<br> schedule_interval=None, # Designed for ad-hoc execution only!<br> tags=[\"Snowflake\", \"KeyPair\", \"PWD\", \"RSA\"],<br>)<br># Define DAG documentation<br>dag.doc_md = \"\"\" This DAG is meant for ad-hoc execution and should be triggered via a DAG Run Config. Example:<br>{<br> \"snowflake_user\":\"xxxxx\",<br> \"email_id\" : \"abc@xyz.com\"<br>},<br>The DAG sends an email to the specified email_id, containing the new RSA keys attached, and BrandsBI in BCC.<br>\"\"\"<br># \u2026 (Continuing with the code)</pre>\n<p>In this code snippet, we import the necessary libraries, define the DAG with its default settings, and provide documentation on its usage. The DAG is intended for ad-hoc execution, triggered via a DAG Run Config that supplies the Snowflake user and email ID as parameters.</p>\n<p><strong>Generating RSA Key\u00a0Pairs</strong></p>\n<p>The core of the automation lies in the Python function responsible for generating RSA key pairs. Here\u2019s an excerpt from the code that performs this\u00a0task:</p>\n<pre># ... (Previous code)<br><br>def generate_rsa_keys(**kwargs):<br>    # Extract the Snowflake user and email ID from the DAG Run Config<br>    snowflake_user = kwargs[\"dag_run\"].conf.get(\"snowflake_user\").strip()<br>    email_id = kwargs[\"dag_run\"].conf.get(\"email_id\").strip()<br><br>    if snowflake_user is None or email_id is None:<br>        raise AirflowException(\"The snowflake_user or email_id is not provided\")<br><br>    # Generate a new private key and passphrase<br>    private_key = rsa.generate_private_key(<br>        public_exponent=65537, key_size=2048, backend=default_backend()<br>    )<br><br>    # Generate a random passphrase<br>    passphrase = generate_passphrase()<br><br>    # Serialize the private key into PEM format<br>    private_key_pem = private_key.private_bytes(<br>        encoding=serialization.Encoding.PEM,<br>        format=serialization.PrivateFormat.PKCS8,<br>        encryption_algorithm=serialization.BestAvailableEncryption(passphrase),<br>    )<br><br>    # Retrieve the corresponding public key<br>    public_key = private_key.public_key()<br><br>    # Serialize the public key into PEM format without delimiters<br>    public_key_pem_without_delimiters = (<br>        public_key.public_bytes(<br>            encoding=serialization.Encoding.PEM,<br>            format=serialization.PublicFormat.SubjectPublicKeyInfo,<br>        )<br>        .decode(\"utf-8\")<br>        .split(\"\\n\")[1:-2]<br>    )  # Excluding the first and last lines<br><br>    # Convert the public key into a string without delimiters<br>    public_key_string = \"\".join(public_key_pem_without_delimiters)<br><br>    # Modify the Snowflake user with the new public key<br>    operation_result = alter_sf_user(public_key_string, snowflake_user)<br><br>    if operation_result is False:<br>        raise AirflowException(\"Failed to alter the user\")<br><br>    # Send an email with the new RSA keys<br>    html_content = f\"\"\"<br>    &lt;!DOCTYPE html&gt;<br>    &lt;!-- (Email content) --&gt;<br>    \"\"\"<br>    # ... (Email sending)<br><br># ... (Continuing with the code)</pre>\n<pre>def generate_passphrase(length=16):<br>    import random<br>    import string<br><br>    # Define character sets for each category<br>    uppercase_letters = string.ascii_uppercase<br>    lowercase_letters = string.ascii_lowercase<br>    digits = string.digits<br>    special_characters = \"!@#$%^&amp;*()_-+=&lt;&gt;?/\\\\\"<br><br>    # Ensure that the passphrase meets the minimum length requirement<br>    if length &lt; 12:<br>        raise ValueError(\"Passphrase length must be at least 12 characters.\")<br><br>    # Create a list of characters to choose from<br>    character_set = uppercase_letters + lowercase_letters + digits + special_characters<br><br>    # Ensure at least one character from each category<br>    passphrase = (<br>        random.choice(uppercase_letters)<br>        + random.choice(lowercase_letters)<br>        + random.choice(digits)<br>        + random.choice(special_characters)<br>    )<br><br>    # Generate the remaining characters in the passphrase<br>    for _ in range(length - 4):<br>        passphrase += random.choice(character_set)<br><br>    # Shuffle the characters in the passphrase<br>    passphrase_list = list(passphrase)<br>    random.shuffle(passphrase_list)<br>    passphrase = \"\".join(passphrase_list)<br><br>    return passphrase.encode(\"utf-8\")<br><br>def alter_sf_user(public_key_string: str, snowflake_user: str) -&gt; bool:<br>    from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook<br><br>    snow_hook = SnowflakeHook(<br>        \"snowflake_conn\",<br>    )<br><br>    op = snow_hook.run(<br>        f\"ALTER USER {snowflake_user} SET RSA_PUBLIC_KEY = '{public_key_string}';\"<br>    )<br><br>    if op is not None:<br>        return False<br>    return True</pre>\n<p>In this function, we generate a new private key, extract the Snowflake user and email ID from the DAG Run Config, and modify the Snowflake user by setting the new public key. Finally, we send an email to the specified email ID, including the new RSA keys and passphrase.</p>\n<p><strong>In Conclusion</strong></p>\n<p>Automating the rotation of RSA key pairs for Snowflake users using Apache Airflow simplifies the security process and ensures that keys are frequently updated. This approach reduces the risks associated with prolonged key usage and enhances the overall security of your Snowflake data warehouse.</p>\n<p>By following the steps outlined in this guide and adapting the code to your specific Snowflake configuration, you can implement an effective and secure key pair rotation process in your organization. This approach not only enhances security but also minimizes the operational overhead typically associated with manual key management.</p>\n<p>Remember to schedule this Airflow DAG as needed or trigger it manually to ensure that your Snowflake users continually possess up-to-date and secure key pairs for authentication.</p>\n<p><strong>Useful Links</strong></p>\n<ul>\n<li><a href=\"https://docs.snowflake.com/en/user-guide/key-pair-auth\">https://docs.snowflake.com/en/user-guide/key-pair-auth</a></li>\n<li><a href=\"https://community.snowflake.com/s/article/How-To-Connect-to-Snowflake-using-key-pair-authentication-directly-using-the-private-key-incode-with-the-Python-Connector\">https://community.snowflake.com/s/article/How-To-Connect-to-Snowflake-using-key-pair-authentication-directly-using-the-private-key-incode-with-the-Python-Connector</a></li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=4557f998b6bf\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["airflow","openssl","snowflake"]},{"title":"Pushing Files from Box to S3 via Airflow","pubDate":"2023-06-17 18:11:13","link":"https://medium.com/@darshan.mohanty14/pushing-files-from-box-to-s3-via-airflow-6e1b3c2dad1a?source=rss-a0040ce85d9d------2","guid":"https://medium.com/p/6e1b3c2dad1a","author":"Priyadarshan Mohanty","thumbnail":"https://cdn-images-1.medium.com/max/1024/1*iflc6pP2rEgTpTzTF4nuTw.png","description":"\n<h3>Introduction</h3>\n<p>In this tutorial, we will explore how to leverage Apache Airflow to transfer files from Box to Amazon S3. We\u2019ll walk through the process of setting up a Box Custom App, configuring Airflow connections, creating a BoxHook, and developing the BoxtoS3Operator. By following these steps, you\u2019ll be able to automate the transfer of files from Box to S3 using\u00a0Airflow.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*iflc6pP2rEgTpTzTF4nuTw.png\"></figure><h3>Prerequisites</h3>\n<p>Before we begin, make sure you have the following prerequisites in\u00a0place:</p>\n<ul>\n<li>A Box.com account to access Box\u2019s content management system.</li>\n<li>An existing Box Custom App with the necessary permissions. We\u2019ll be using the Custom App for authentication and accessing the Box\u00a0API.</li>\n<li>Basic knowledge of Apache Airflow, including how to create and configure DAGs.</li>\n</ul>\n<h3>Authentication with\u00a0Box</h3>\n<p>Custom Apps encompass most use cases and is the most flexible application type.</p>\n<p>A Custom App is best used when the application:</p>\n<ul>\n<li>Wants to use OAuth 2.0, JWT, or Client Credentials Grant for authentication.</li>\n<li>Wants to upload and download\u00a0files</li>\n<li>Wants the freedom to access both their own files, as well as files owned by managed or external\u00a0users.</li>\n<li>Wants the option to list the application in the Box App\u00a0Center</li>\n<li>Wants to provide integration into the Box Web\u00a0App</li>\n</ul>\n<p>Custom Apps support <a href=\"https://developer.box.com/guides/authentication/oauth2/\">OAuth 2.0</a>, <a href=\"https://developer.box.com/guides/authentication/jwt/\">JWT</a>, and <a href=\"https://developer.box.com/guides/authentication/client-credentials//\">Client Credentials Grant</a>.</p>\n<h4>JWT:</h4>\n<ul>\n<li>Best for building integrations or apps with external collaborators</li>\n<li>The app authenticates using a JSON Web Token. Requires a public/private key-pair for added security.</li>\n</ul>\n<h4>OAuth 2.0:</h4>\n<ul>\n<li>Best for building mobile or web\u00a0apps</li>\n<li>Users must authenticate with their Box\u00a0login.</li>\n</ul>\n<h4>Client Credentials Grant:</h4>\n<ul>\n<li>Best for scripting and back office integrations</li>\n<li>The app authenticates using Client ID and Client\u00a0Secret.</li>\n</ul>\n<p>Server-side authentication using JSON Web Tokens (JWT) is the most common way to authenticate to the Box API. Server-side authentication using JWT is only available to the Custom Application <a href=\"https://developer.box.com/guides/applications/select/\">app\u00a0type</a>.</p>\n<p>There are two ways you can verify an application\u2019s permissions:</p>\n<ul>\n<li>using public and private key\u00a0pair</li>\n<li>using a client id and client secret (<a href=\"https://developer.box.com/guides/authentication/client-credentials/\">Client Credentials Grant</a>)</li>\n</ul>\n<p>We can go with either Server Side authentication using JWT or CCG. In this approach, we chose CCG as it\u2019s the fastest and easiest way to prototype or script against your Box enterprise.</p>\n<h3>Setting up the Box Custom\u00a0App</h3>\n<ol>\n<li>Go to <a href=\"https://cloud.app.box.com/developers/console\">https://cloud.app.box.com/developers/console</a> and create a new\u00a0app.</li>\n<li>Select <strong>Custom App</strong> from the list of application types. A modal will appear to prompt a selection for the next\u00a0step.</li>\n<li>Select <strong>Server Authentication (with Client Credentials Grant)</strong> if you would like to verify application identity with a client ID and client secret. Then, provide a name for your application and click <strong>Create\u00a0App</strong>.</li>\n<li>Before the application can be used, a Box Admin needs to authorize the application within the Box Admin\u00a0Console.</li>\n<li>Navigate to the <strong>Authorization</strong> tab for your application within the Developer Console.</li>\n</ol>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/750/0*uSFHBL1o1TLdX_Pz.png\"></figure><p>6. Click <strong>Review and Submit</strong> to send an email to your Box enterprise Admin for approval.</p>\n<p>7. After creation and approval from your Enterprise Admin\u00a0, the dashboard-&gt;configuration will provide the configuration such as client id, tokens,\u00a0etc.</p>\n<p>8. To access or query a folder/file, you must share that folder/file with the app\u2019s service account email. This is found in the Box Dev Console &gt; Your App &gt; General Settings &gt; Service Account Info. Copy that email address and share the desired folder/file with the\u00a0address.</p>\n<p>9. Add the credentials to your Airflow Connections</p>\n<h4>Building the\u00a0BoxHook</h4>\n<p>To interact with the Box API, we\u2019ll create a BoxHook class. This hook will handle the authentication process and provide methods for downloading files and retrieving folder\u00a0items.</p>\n<pre>from airflow.hooks.base_hook import BaseHook<br>from airflow.models import Variable<br>from boxsdk import CCGAuth, Client<br>import json<br>import logging<br>from boxsdk.object.item import Item<br>from typing import Iterable<br><br>log = logging.getLogger(__name__)<br>logging.getLogger(\"boxsdk\").setLevel(logging.CRITICAL)<br><br><br>class BoxHook(BaseHook):<br>    \"\"\"<br>    Wrap around the Box Python SDK<br>    \"\"\"<br><br>    def __init__(self, *args, **kwargs) -&gt; None:<br>        self.client = None<br>        super().__init__(*args, **kwargs)<br><br>    def get_conn(self) -&gt; Client:<br>        # Get Box connection details from Airflow Connection<br>        client_secret = BaseHook.get_connection(\"box_conn\").get_password()<br>        client_id = BaseHook.get_connection(\"box_conn\").login<br>        extra = json.loads(BaseHook.get_connection(\"box_conn\").get_extra())<br><br>        # Authenticate and create a Box client<br>        auth = CCGAuth(<br>            client_id=client_id,<br>            client_secret=client_secret,<br>            user=extra[\"user\"]<br>        )<br>        client = Client(auth)<br>        user = client.user().get()<br>        log.info(\"Current User is {} and User Id is {}\".format(user.name, user.id))<br>        return client<br><br>    def download_file(self, file_id: str, file_name: str) -&gt; None:<br>        \"\"\"<br>        Download a file from Box given its ID and save it with the provided file name.<br>        \"\"\"<br>        client = self.get_conn()<br>        client.file(file_id=file_id).download_to(file_name)<br><br>    def get_folder_items(self, folder_id: str) -&gt; Iterable[Item]:<br>        \"\"\"<br>        Retrieve a list of items within a Box folder given its ID.<br>        \"\"\"<br>        client = self.get_conn()<br>        root_folder = client.folder(folder_id=folder_id).get()<br>        items = root_folder.get_items()<br>        return items</pre>\n<h4>Creating the BoxtoS3Operator</h4>\n<p>To transfer files from Box to S3, we\u2019ll create a BoxtoS3Operator. This operator will utilize the BoxHook to download files locally and then upload them to\u00a0S3.</p>\n<pre>import logging<br>import os<br>import tempfile<br>from datetime import datetime<br>from typing import Optional<br><br>import pendulum<br>from airflow.exceptions import AirflowException<br>from airflow.hooks.S3_hook import S3Hook<br>from airflow.models import BaseOperator, Variable<br>from airflow.plugins_manager import AirflowPlugin<br>from airflow.utils.decorators import apply_defaults<br>from boxsdk import Client<br>from hooks.BoxHook import BoxHook<br><br>est_tz = pendulum.timezone(\"EST5EDT\")<br>log = logging.getLogger(__name__)<br>logging.getLogger(\"boxsdk\").setLevel(logging.CRITICAL)<br><br>S3_HISTORY = f\"default_bucket\"<br><br><br>class BoxtoS3Operator(BaseOperator):<br>    \"\"\"<br>    Downloads files/folders from Box and uploads them to S3.<br><br>    :param choice: The choice between 'file' or 'folder'.<br>    :type choice: str<br>    :param input_box_folder_id: The folder ID of the Box location.<br>    :type input_box_folder_id: str, optional<br>    :param input_box_file_id: The file ID of the Box location.<br>    :type input_box_file_id: str, optional<br>    :param output_s3_key: The S3 key to store the downloaded data.<br>    :type output_s3_key: str, optional<br>    :param output_s3_bucket: The S3 bucket to store the downloaded data.<br>    :type output_s3_bucket: str, optional<br>    \"\"\"<br><br>    ui_color = \"#c8f1eb\"<br><br>    @apply_defaults<br>    def __init__(<br>        self,<br>        choice: str,<br>        input_box_folder_id: Optional[str] = None,<br>        input_box_file_id: Optional[str] = None,<br>        output_s3_key: Optional[str] = None,<br>        output_s3_bucket: Optional[str] = None,<br>        *args,<br>        **kwargs,<br>    ):<br>        super().__init__(*args, **kwargs)<br>        self.choice = choice<br>        self.input_box_folder_id = input_box_folder_id<br>        self.input_box_file_id = input_box_file_id<br>        self.output_s3_key = output_s3_key<br>        self.output_s3_bucket = output_s3_bucket or S3_HISTORY<br><br>    def __generate_dttm(self) -&gt; str:<br>        \"\"\"Generate a timestamp string in the EST timezone.\"\"\"<br>        return datetime.now(est_tz).strftime(\"%Y%m%d%H%M\")<br><br>    def __box_to_local(self, box_client: Client, file_name: str, file_id: str) -&gt; None:<br>        \"\"\"Download a file from Box and save it locally.\"\"\"<br>        path = os.getcwd()<br>        file_path = os.path.join(path, file_name)<br>        log.info(\"File path is {}\".format(file_path))<br>        with open(file_path, \"wb\") as open_file:<br>            box_client.file(file_id).download_to(open_file)<br><br>    def __local_to_s3(self, s3_client: S3Hook, dttm: str, file_name: str) -&gt; None:<br>        \"\"\"Upload a local file to S3.\"\"\"<br>        s3_key = \"{}/dttm={}/{}\".format(self.output_s3_key, dttm, file_name)<br>        s3_client.load_file(<br>            filename=file_name, key=s3_key, bucket_name=self.output_s3_bucket<br>        )<br>        log.info(f\"{file_name} copied to {s3_key}\")<br><br>    def __validate_parameters(self) -&gt; Optional[str]:<br>        \"\"\"Validate the operator's parameters.\"\"\"<br>        if self.choice not in [\"file\", \"folder\"]:<br>            return \"Invalid choice. Select between 'file' or 'folder'.\"<br>        elif self.choice == \"folder\" and not self.input_box_folder_id:<br>            return \"Folder ID is not provided.\"<br>        elif self.choice == \"file\" and not self.input_box_file_id:<br>            return \"File ID is not provided.\"<br>        return None<br><br>    def execute(self, context):<br>        err = self.__validate_parameters()<br>        if err:<br>            raise AirflowException(err)<br><br>        bh = BoxHook()<br>        client = bh.get_conn()<br>        user = client.user().get()<br>        log.info(\"Current User is {} and User ID is {}\".format(user.name, user.id))<br>        dttm = self.__generate_dttm()<br>        log.info(\"Generated Dttm: {}\".format(dttm))<br><br>        with tempfile.TemporaryDirectory() as tmp_dir:<br>            os.chdir(tmp_dir)<br>            log.info(\"Current working temp directory: {}\".format(tmp_dir))<br><br>            items = (<br>                bh.get_folder_items(self.input_box_folder_id)<br>                if self.choice == \"folder\"<br>                else [client.file(self.input_box_file_id).get()]<br>            )<br>            client_hook = S3Hook(\"s3_conn\")<br>            for item in items:<br>                log.info(<br>                    '{0} {1} is named \"{2}\"'.format(<br>                        item.type.capitalize(), item.id, item.name<br>                    )<br>                )<br><br>                self.__box_to_local(client, item.name, item.id)<br>                self.__local_to_s3(client_hook, dttm, item.name)<br><br>                os.remove(item.name)<br><br><br>class ETLBoxtoS3Operator(AirflowPlugin):<br>    name = \"BoxtoS3Operator\"<br>    operators = [BoxtoS3Operator]</pre>\n<h3>Configuring the Airflow\u00a0DAG</h3>\n<p>To use the BoxtoS3Operator, configure an Airflow DAG as\u00a0follows:</p>\n<pre><br>from datetime import datetime<br>from airflow import DAG<br>from operators import BoxtoS3Operator<br><br>default_args = {<br>    \"owner\": \"your_name\",<br>    \"depends_on_past\": False,<br>    \"start_date\": datetime(2023, 6, 1),<br>    \"retries\": 1,<br>}<br><br>with DAG(\"box_to_s3_dag\", default_args=default_args, schedule_interval=\"0 0 * * *\") as dag:<br>    box_to_s3_task = BoxtoS3Operator(<br>        task_id=\"box_to_s3_transfer\",<br>        box_folder_id=\"your_box_folder_id\",<br>        s3_bucket=\"your_s3_bucket\",<br>    )<br><br>    box_to_s3_task</pre>\n<h3>Conclusion</h3>\n<p>In this tutorial, we covered the process of pushing files from Box to S3 using Apache Airflow. We discussed the prerequisites, set up a Box Custom App, configured Airflow connections, created a BoxHook for Box API interactions, and developed a BoxtoS3Operator for file transfer. With this knowledge, you can now automate the transfer of files between Box and S3 using Airflow, saving time and effort in managing your cloud storage. You call also use these files in S3 for doing pre-processing or loading to any RDBMS\u2019 or Cloud Datawarehouses for analysis.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6e1b3c2dad1a\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Introduction</h3>\n<p>In this tutorial, we will explore how to leverage Apache Airflow to transfer files from Box to Amazon S3. We\u2019ll walk through the process of setting up a Box Custom App, configuring Airflow connections, creating a BoxHook, and developing the BoxtoS3Operator. By following these steps, you\u2019ll be able to automate the transfer of files from Box to S3 using\u00a0Airflow.</p>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/1024/1*iflc6pP2rEgTpTzTF4nuTw.png\"></figure><h3>Prerequisites</h3>\n<p>Before we begin, make sure you have the following prerequisites in\u00a0place:</p>\n<ul>\n<li>A Box.com account to access Box\u2019s content management system.</li>\n<li>An existing Box Custom App with the necessary permissions. We\u2019ll be using the Custom App for authentication and accessing the Box\u00a0API.</li>\n<li>Basic knowledge of Apache Airflow, including how to create and configure DAGs.</li>\n</ul>\n<h3>Authentication with\u00a0Box</h3>\n<p>Custom Apps encompass most use cases and is the most flexible application type.</p>\n<p>A Custom App is best used when the application:</p>\n<ul>\n<li>Wants to use OAuth 2.0, JWT, or Client Credentials Grant for authentication.</li>\n<li>Wants to upload and download\u00a0files</li>\n<li>Wants the freedom to access both their own files, as well as files owned by managed or external\u00a0users.</li>\n<li>Wants the option to list the application in the Box App\u00a0Center</li>\n<li>Wants to provide integration into the Box Web\u00a0App</li>\n</ul>\n<p>Custom Apps support <a href=\"https://developer.box.com/guides/authentication/oauth2/\">OAuth 2.0</a>, <a href=\"https://developer.box.com/guides/authentication/jwt/\">JWT</a>, and <a href=\"https://developer.box.com/guides/authentication/client-credentials//\">Client Credentials Grant</a>.</p>\n<h4>JWT:</h4>\n<ul>\n<li>Best for building integrations or apps with external collaborators</li>\n<li>The app authenticates using a JSON Web Token. Requires a public/private key-pair for added security.</li>\n</ul>\n<h4>OAuth 2.0:</h4>\n<ul>\n<li>Best for building mobile or web\u00a0apps</li>\n<li>Users must authenticate with their Box\u00a0login.</li>\n</ul>\n<h4>Client Credentials Grant:</h4>\n<ul>\n<li>Best for scripting and back office integrations</li>\n<li>The app authenticates using Client ID and Client\u00a0Secret.</li>\n</ul>\n<p>Server-side authentication using JSON Web Tokens (JWT) is the most common way to authenticate to the Box API. Server-side authentication using JWT is only available to the Custom Application <a href=\"https://developer.box.com/guides/applications/select/\">app\u00a0type</a>.</p>\n<p>There are two ways you can verify an application\u2019s permissions:</p>\n<ul>\n<li>using public and private key\u00a0pair</li>\n<li>using a client id and client secret (<a href=\"https://developer.box.com/guides/authentication/client-credentials/\">Client Credentials Grant</a>)</li>\n</ul>\n<p>We can go with either Server Side authentication using JWT or CCG. In this approach, we chose CCG as it\u2019s the fastest and easiest way to prototype or script against your Box enterprise.</p>\n<h3>Setting up the Box Custom\u00a0App</h3>\n<ol>\n<li>Go to <a href=\"https://cloud.app.box.com/developers/console\">https://cloud.app.box.com/developers/console</a> and create a new\u00a0app.</li>\n<li>Select <strong>Custom App</strong> from the list of application types. A modal will appear to prompt a selection for the next\u00a0step.</li>\n<li>Select <strong>Server Authentication (with Client Credentials Grant)</strong> if you would like to verify application identity with a client ID and client secret. Then, provide a name for your application and click <strong>Create\u00a0App</strong>.</li>\n<li>Before the application can be used, a Box Admin needs to authorize the application within the Box Admin\u00a0Console.</li>\n<li>Navigate to the <strong>Authorization</strong> tab for your application within the Developer Console.</li>\n</ol>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/750/0*uSFHBL1o1TLdX_Pz.png\"></figure><p>6. Click <strong>Review and Submit</strong> to send an email to your Box enterprise Admin for approval.</p>\n<p>7. After creation and approval from your Enterprise Admin\u00a0, the dashboard-&gt;configuration will provide the configuration such as client id, tokens,\u00a0etc.</p>\n<p>8. To access or query a folder/file, you must share that folder/file with the app\u2019s service account email. This is found in the Box Dev Console &gt; Your App &gt; General Settings &gt; Service Account Info. Copy that email address and share the desired folder/file with the\u00a0address.</p>\n<p>9. Add the credentials to your Airflow Connections</p>\n<h4>Building the\u00a0BoxHook</h4>\n<p>To interact with the Box API, we\u2019ll create a BoxHook class. This hook will handle the authentication process and provide methods for downloading files and retrieving folder\u00a0items.</p>\n<pre>from airflow.hooks.base_hook import BaseHook<br>from airflow.models import Variable<br>from boxsdk import CCGAuth, Client<br>import json<br>import logging<br>from boxsdk.object.item import Item<br>from typing import Iterable<br><br>log = logging.getLogger(__name__)<br>logging.getLogger(\"boxsdk\").setLevel(logging.CRITICAL)<br><br><br>class BoxHook(BaseHook):<br>    \"\"\"<br>    Wrap around the Box Python SDK<br>    \"\"\"<br><br>    def __init__(self, *args, **kwargs) -&gt; None:<br>        self.client = None<br>        super().__init__(*args, **kwargs)<br><br>    def get_conn(self) -&gt; Client:<br>        # Get Box connection details from Airflow Connection<br>        client_secret = BaseHook.get_connection(\"box_conn\").get_password()<br>        client_id = BaseHook.get_connection(\"box_conn\").login<br>        extra = json.loads(BaseHook.get_connection(\"box_conn\").get_extra())<br><br>        # Authenticate and create a Box client<br>        auth = CCGAuth(<br>            client_id=client_id,<br>            client_secret=client_secret,<br>            user=extra[\"user\"]<br>        )<br>        client = Client(auth)<br>        user = client.user().get()<br>        log.info(\"Current User is {} and User Id is {}\".format(user.name, user.id))<br>        return client<br><br>    def download_file(self, file_id: str, file_name: str) -&gt; None:<br>        \"\"\"<br>        Download a file from Box given its ID and save it with the provided file name.<br>        \"\"\"<br>        client = self.get_conn()<br>        client.file(file_id=file_id).download_to(file_name)<br><br>    def get_folder_items(self, folder_id: str) -&gt; Iterable[Item]:<br>        \"\"\"<br>        Retrieve a list of items within a Box folder given its ID.<br>        \"\"\"<br>        client = self.get_conn()<br>        root_folder = client.folder(folder_id=folder_id).get()<br>        items = root_folder.get_items()<br>        return items</pre>\n<h4>Creating the BoxtoS3Operator</h4>\n<p>To transfer files from Box to S3, we\u2019ll create a BoxtoS3Operator. This operator will utilize the BoxHook to download files locally and then upload them to\u00a0S3.</p>\n<pre>import logging<br>import os<br>import tempfile<br>from datetime import datetime<br>from typing import Optional<br><br>import pendulum<br>from airflow.exceptions import AirflowException<br>from airflow.hooks.S3_hook import S3Hook<br>from airflow.models import BaseOperator, Variable<br>from airflow.plugins_manager import AirflowPlugin<br>from airflow.utils.decorators import apply_defaults<br>from boxsdk import Client<br>from hooks.BoxHook import BoxHook<br><br>est_tz = pendulum.timezone(\"EST5EDT\")<br>log = logging.getLogger(__name__)<br>logging.getLogger(\"boxsdk\").setLevel(logging.CRITICAL)<br><br>S3_HISTORY = f\"default_bucket\"<br><br><br>class BoxtoS3Operator(BaseOperator):<br>    \"\"\"<br>    Downloads files/folders from Box and uploads them to S3.<br><br>    :param choice: The choice between 'file' or 'folder'.<br>    :type choice: str<br>    :param input_box_folder_id: The folder ID of the Box location.<br>    :type input_box_folder_id: str, optional<br>    :param input_box_file_id: The file ID of the Box location.<br>    :type input_box_file_id: str, optional<br>    :param output_s3_key: The S3 key to store the downloaded data.<br>    :type output_s3_key: str, optional<br>    :param output_s3_bucket: The S3 bucket to store the downloaded data.<br>    :type output_s3_bucket: str, optional<br>    \"\"\"<br><br>    ui_color = \"#c8f1eb\"<br><br>    @apply_defaults<br>    def __init__(<br>        self,<br>        choice: str,<br>        input_box_folder_id: Optional[str] = None,<br>        input_box_file_id: Optional[str] = None,<br>        output_s3_key: Optional[str] = None,<br>        output_s3_bucket: Optional[str] = None,<br>        *args,<br>        **kwargs,<br>    ):<br>        super().__init__(*args, **kwargs)<br>        self.choice = choice<br>        self.input_box_folder_id = input_box_folder_id<br>        self.input_box_file_id = input_box_file_id<br>        self.output_s3_key = output_s3_key<br>        self.output_s3_bucket = output_s3_bucket or S3_HISTORY<br><br>    def __generate_dttm(self) -&gt; str:<br>        \"\"\"Generate a timestamp string in the EST timezone.\"\"\"<br>        return datetime.now(est_tz).strftime(\"%Y%m%d%H%M\")<br><br>    def __box_to_local(self, box_client: Client, file_name: str, file_id: str) -&gt; None:<br>        \"\"\"Download a file from Box and save it locally.\"\"\"<br>        path = os.getcwd()<br>        file_path = os.path.join(path, file_name)<br>        log.info(\"File path is {}\".format(file_path))<br>        with open(file_path, \"wb\") as open_file:<br>            box_client.file(file_id).download_to(open_file)<br><br>    def __local_to_s3(self, s3_client: S3Hook, dttm: str, file_name: str) -&gt; None:<br>        \"\"\"Upload a local file to S3.\"\"\"<br>        s3_key = \"{}/dttm={}/{}\".format(self.output_s3_key, dttm, file_name)<br>        s3_client.load_file(<br>            filename=file_name, key=s3_key, bucket_name=self.output_s3_bucket<br>        )<br>        log.info(f\"{file_name} copied to {s3_key}\")<br><br>    def __validate_parameters(self) -&gt; Optional[str]:<br>        \"\"\"Validate the operator's parameters.\"\"\"<br>        if self.choice not in [\"file\", \"folder\"]:<br>            return \"Invalid choice. Select between 'file' or 'folder'.\"<br>        elif self.choice == \"folder\" and not self.input_box_folder_id:<br>            return \"Folder ID is not provided.\"<br>        elif self.choice == \"file\" and not self.input_box_file_id:<br>            return \"File ID is not provided.\"<br>        return None<br><br>    def execute(self, context):<br>        err = self.__validate_parameters()<br>        if err:<br>            raise AirflowException(err)<br><br>        bh = BoxHook()<br>        client = bh.get_conn()<br>        user = client.user().get()<br>        log.info(\"Current User is {} and User ID is {}\".format(user.name, user.id))<br>        dttm = self.__generate_dttm()<br>        log.info(\"Generated Dttm: {}\".format(dttm))<br><br>        with tempfile.TemporaryDirectory() as tmp_dir:<br>            os.chdir(tmp_dir)<br>            log.info(\"Current working temp directory: {}\".format(tmp_dir))<br><br>            items = (<br>                bh.get_folder_items(self.input_box_folder_id)<br>                if self.choice == \"folder\"<br>                else [client.file(self.input_box_file_id).get()]<br>            )<br>            client_hook = S3Hook(\"s3_conn\")<br>            for item in items:<br>                log.info(<br>                    '{0} {1} is named \"{2}\"'.format(<br>                        item.type.capitalize(), item.id, item.name<br>                    )<br>                )<br><br>                self.__box_to_local(client, item.name, item.id)<br>                self.__local_to_s3(client_hook, dttm, item.name)<br><br>                os.remove(item.name)<br><br><br>class ETLBoxtoS3Operator(AirflowPlugin):<br>    name = \"BoxtoS3Operator\"<br>    operators = [BoxtoS3Operator]</pre>\n<h3>Configuring the Airflow\u00a0DAG</h3>\n<p>To use the BoxtoS3Operator, configure an Airflow DAG as\u00a0follows:</p>\n<pre><br>from datetime import datetime<br>from airflow import DAG<br>from operators import BoxtoS3Operator<br><br>default_args = {<br>    \"owner\": \"your_name\",<br>    \"depends_on_past\": False,<br>    \"start_date\": datetime(2023, 6, 1),<br>    \"retries\": 1,<br>}<br><br>with DAG(\"box_to_s3_dag\", default_args=default_args, schedule_interval=\"0 0 * * *\") as dag:<br>    box_to_s3_task = BoxtoS3Operator(<br>        task_id=\"box_to_s3_transfer\",<br>        box_folder_id=\"your_box_folder_id\",<br>        s3_bucket=\"your_s3_bucket\",<br>    )<br><br>    box_to_s3_task</pre>\n<h3>Conclusion</h3>\n<p>In this tutorial, we covered the process of pushing files from Box to S3 using Apache Airflow. We discussed the prerequisites, set up a Box Custom App, configured Airflow connections, created a BoxHook for Box API interactions, and developed a BoxtoS3Operator for file transfer. With this knowledge, you can now automate the transfer of files between Box and S3 using Airflow, saving time and effort in managing your cloud storage. You call also use these files in S3 for doing pre-processing or loading to any RDBMS\u2019 or Cloud Datawarehouses for analysis.</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=6e1b3c2dad1a\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["box","s3","airflow"]},{"title":"Introduction to Stream Processing using Kafka Streams","pubDate":"2020-07-05 07:44:56","link":"https://medium.com/@darshan.mohanty14/introduction-to-stream-processing-using-kafka-streams-5e02ae6f9268?source=rss-a0040ce85d9d------2","guid":"https://medium.com/p/5e02ae6f9268","author":"Priyadarshan Mohanty","thumbnail":"https://cdn-images-1.medium.com/max/800/0*KvwBlUiEneVAwiqL.png","description":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/0*KvwBlUiEneVAwiqL.png\"></figure><p>Kafka Streams is a Java library developed to help applications that do stream processing built on Kafka. To learn about Kafka Streams, you need to have a basic idea about Kafka to understand better. If you\u2019ve worked with Kafka before, Kafka Streams is going to be easy to understand; otherwise, you can look up some basic tutorials online to get going with this\u00a0article.</p>\n<p>Let us get started with some highlights of Kafka\u00a0Streams:</p>\n<ul>\n<li>\n<strong>Low Barrier to Entry</strong>: Quickly write and run a small-scale POC on a single instance. You only need to run multiple instances of the application on various machines to scale up to high-volume production workloads.</li>\n<li>\n<strong>Lightweight and straightforward client library:</strong> Can be easily embedded in any Java application and integrated with any existing packaging, deployment, and operational tools.</li>\n<li>\n<strong>No external dependencies on systems</strong> other than Apache Kafka\u00a0itself</li>\n<li>\n<strong>Fault-tolerant local state:</strong> Enables fast and efficient stateful operations like windowed joins and aggregations</li>\n<li>\n<strong>Supports exactly-once processing:</strong> Each record will be processed once and only once, even when there is a\u00a0failure.</li>\n<li>\n<strong>One-record-at-a-time processing</strong> to achieve millisecond processing latency supports event-time-based windowing operations with out-of-order arrival of\u00a0records.</li>\n</ul>\n<h3>Key Concepts:</h3>\n<p><strong>Stream</strong>: An ordered, replayable, and fault-tolerant sequence of immutable data records, where each data record is defined as a key-value pair.</p>\n<p><strong>Stream Processor</strong>: A node in the processor topology represents a processing step to transform data in streams by receiving one input record at a time from its source in the topology, applying any operation to it, and may subsequently produce one or more output records to its sinks. There are two individual processors in the topology:</p>\n<ul>\n<li>\n<strong>Source Processor</strong>: A source processor is a special type of stream processor that does not have any upstream processors. It produces an input stream to its topology from one or multiple Kafka topics by consuming records from these topics and forwarding them to its down-stream processors.</li>\n<li>\n<strong>Sink Processor</strong>: A sink processor is a special type of stream processor that does not have down-stream processors. It sends any received records from its up-stream processors to a specified Kafka\u00a0topic.</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/768/0*EJfXNEwdJ7kupW6s.jpg\"><figcaption>Example of a Stream Processor Topology</figcaption></figure><ul>\n<li>\n<strong>KStream</strong>: KStream is nothing but that, a Kafka Stream. It\u2019s a never-ending flow of data in a stream. Each piece of data\u200a\u2014\u200aa record or a fact\u200a\u2014\u200ais a collection of key-value pairs. Data records in a record stream are always interpreted as an \u201cINSERT\u201d.</li>\n<li>\n<strong>KTable</strong>: A KTable is just an abstraction of the stream, where only the latest value is kept. Data records in a record stream are always interpreted as an \u201cUPDATE\u201d.</li>\n</ul>\n<p>There is actually a close relationship between streams and tables, the so-called <a href=\"https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/\">stream-table duality</a>.</p>\n<h3>The Setup:</h3>\n<p>Let\u2019s Start with the Setup using Scala instead of Java. The Kafka Streams DSL for Scala library is a wrapper over the existing Java APIs for Kafka Streams\u00a0DSL.</p>\n<p>You can create a topic using the below commands (need to have Kafka and Zookeeper pre-installed in your\u00a0system)</p>\n<pre>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic inputTopic<br>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic outputTopic</pre>\n<p>To Setup things, we need to create a KafkaStreams Instance. It needs a topology and configuration (java.util.Properties). We also need an input topic and output topic. Let's look through a simple example of sending data from an input topic to an output topic using the Streams\u00a0API</p>\n<pre>val config: Properties = {<br>    val properties = new Properties()<br>    properties.put(StreamsConfig.APPLICATION_ID_CONFIG, \"your-application\")<br>    properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\")<br>    properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"latest\")<br>    properties.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE)<br>    properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String())<br>    properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String())<br>    properties<br>    }</pre>\n<p>StreamsBuilder provides the high-level Kafka Streams DSL to specify a Kafka Streams topology.</p>\n<pre>val builder: StreamsBuilder = new StreamsBuilder</pre>\n<p>Creates a KStream from the specified topics.</p>\n<pre>val inputStream: KStream[String,String] = builder.stream(inputTopic, Consumed.`with`(Serdes.String(), Serdes.String()))</pre>\n<p>Store the input stream to the output\u00a0topic.</p>\n<pre>inputStream.to(outputTopic)(producedFromSerde(Serdes.String(),Serdes.String())</pre>\n<p>Starts the Streams Application with a shutdown\u00a0hook</p>\n<pre>val kEventStream = new KafkaStreams(builder.build(), config)<br>kEventStream.start()<br>sys.ShutdownHookThread {<br>      kEventStream.close(10, TimeUnit.SECONDS)<br>    }</pre>\n<p>You can send data to the input topic using or programmatically</p>\n<pre>kafka-console-producer --broker-list localhost:9092 --topic inputTopic</pre>\n<p>And can fetch the data from the output topic\u00a0using</p>\n<pre>kafka-console-consumer --bootstrap-server localhost:9092 --topic outputTopic --from-beginning</pre>\n<p>You can add the necessary dependencies in your build file for sbt or pom file for maven. Below is an example for build.sbt.</p>\n<pre>// Kafka<br>libraryDependencies += \"org.apache.kafka\" %% \"kafka-streams-scala\" % \"2.0.0\"<br>libraryDependencies += \"javax.ws.rs\" % \"javax.ws.rs-api\" % \"2.1\" artifacts( Artifact(\"javax.ws.rs-api\", \"jar\", \"jar\")) // this is a workaround. There is an upstream dependency that causes trouble in SBT builds.</pre>\n<p>Let us modify the code a little bit to try out a WordCount example: The code splits the sentences into words and groups by word as a key and the number of occurrences or count as value and is being sent to the output topic by converting the KTable to\u00a0KStream.</p>\n<pre>val textLines: KStream[String, String] = builder.stream[String, String](inputTopic)<br>val wordCounts: KTable[String, Long] = textLines<br>\t\t.flatMapValues(textLine =&gt; textLine.toLowerCase.split(\"\\\\W+\"))<br>\t\t.groupBy((_, word) =&gt; word)<br>\t\t.count()(materializedFromSerde(Serdes.String(),Serdes.Long()))<br>\twordCounts.toStream.to(outputTopic)(producedFromSerde(Serdes.String(),Serdes.Long())</pre>\n<p>With the above process, we can now implement a simple streaming application or a word count application using Kafka Streams in Scala. Check out the <a href=\"https://kafka.apache.org/documentation/streams/\">documentation</a> on their website for more applications!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5e02ae6f9268\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/800/0*KvwBlUiEneVAwiqL.png\"></figure><p>Kafka Streams is a Java library developed to help applications that do stream processing built on Kafka. To learn about Kafka Streams, you need to have a basic idea about Kafka to understand better. If you\u2019ve worked with Kafka before, Kafka Streams is going to be easy to understand; otherwise, you can look up some basic tutorials online to get going with this\u00a0article.</p>\n<p>Let us get started with some highlights of Kafka\u00a0Streams:</p>\n<ul>\n<li>\n<strong>Low Barrier to Entry</strong>: Quickly write and run a small-scale POC on a single instance. You only need to run multiple instances of the application on various machines to scale up to high-volume production workloads.</li>\n<li>\n<strong>Lightweight and straightforward client library:</strong> Can be easily embedded in any Java application and integrated with any existing packaging, deployment, and operational tools.</li>\n<li>\n<strong>No external dependencies on systems</strong> other than Apache Kafka\u00a0itself</li>\n<li>\n<strong>Fault-tolerant local state:</strong> Enables fast and efficient stateful operations like windowed joins and aggregations</li>\n<li>\n<strong>Supports exactly-once processing:</strong> Each record will be processed once and only once, even when there is a\u00a0failure.</li>\n<li>\n<strong>One-record-at-a-time processing</strong> to achieve millisecond processing latency supports event-time-based windowing operations with out-of-order arrival of\u00a0records.</li>\n</ul>\n<h3>Key Concepts:</h3>\n<p><strong>Stream</strong>: An ordered, replayable, and fault-tolerant sequence of immutable data records, where each data record is defined as a key-value pair.</p>\n<p><strong>Stream Processor</strong>: A node in the processor topology represents a processing step to transform data in streams by receiving one input record at a time from its source in the topology, applying any operation to it, and may subsequently produce one or more output records to its sinks. There are two individual processors in the topology:</p>\n<ul>\n<li>\n<strong>Source Processor</strong>: A source processor is a special type of stream processor that does not have any upstream processors. It produces an input stream to its topology from one or multiple Kafka topics by consuming records from these topics and forwarding them to its down-stream processors.</li>\n<li>\n<strong>Sink Processor</strong>: A sink processor is a special type of stream processor that does not have down-stream processors. It sends any received records from its up-stream processors to a specified Kafka\u00a0topic.</li>\n</ul>\n<figure><img alt=\"\" src=\"https://cdn-images-1.medium.com/max/768/0*EJfXNEwdJ7kupW6s.jpg\"><figcaption>Example of a Stream Processor Topology</figcaption></figure><ul>\n<li>\n<strong>KStream</strong>: KStream is nothing but that, a Kafka Stream. It\u2019s a never-ending flow of data in a stream. Each piece of data\u200a\u2014\u200aa record or a fact\u200a\u2014\u200ais a collection of key-value pairs. Data records in a record stream are always interpreted as an \u201cINSERT\u201d.</li>\n<li>\n<strong>KTable</strong>: A KTable is just an abstraction of the stream, where only the latest value is kept. Data records in a record stream are always interpreted as an \u201cUPDATE\u201d.</li>\n</ul>\n<p>There is actually a close relationship between streams and tables, the so-called <a href=\"https://www.confluent.io/blog/introducing-kafka-streams-stream-processing-made-simple/\">stream-table duality</a>.</p>\n<h3>The Setup:</h3>\n<p>Let\u2019s Start with the Setup using Scala instead of Java. The Kafka Streams DSL for Scala library is a wrapper over the existing Java APIs for Kafka Streams\u00a0DSL.</p>\n<p>You can create a topic using the below commands (need to have Kafka and Zookeeper pre-installed in your\u00a0system)</p>\n<pre>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic inputTopic<br>kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic outputTopic</pre>\n<p>To Setup things, we need to create a KafkaStreams Instance. It needs a topology and configuration (java.util.Properties). We also need an input topic and output topic. Let's look through a simple example of sending data from an input topic to an output topic using the Streams\u00a0API</p>\n<pre>val config: Properties = {<br>    val properties = new Properties()<br>    properties.put(StreamsConfig.APPLICATION_ID_CONFIG, \"your-application\")<br>    properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \"localhost:9092\")<br>    properties.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"latest\")<br>    properties.put(StreamsConfig.PROCESSING_GUARANTEE_CONFIG, StreamsConfig.EXACTLY_ONCE)<br>    properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String())<br>    properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String())<br>    properties<br>    }</pre>\n<p>StreamsBuilder provides the high-level Kafka Streams DSL to specify a Kafka Streams topology.</p>\n<pre>val builder: StreamsBuilder = new StreamsBuilder</pre>\n<p>Creates a KStream from the specified topics.</p>\n<pre>val inputStream: KStream[String,String] = builder.stream(inputTopic, Consumed.`with`(Serdes.String(), Serdes.String()))</pre>\n<p>Store the input stream to the output\u00a0topic.</p>\n<pre>inputStream.to(outputTopic)(producedFromSerde(Serdes.String(),Serdes.String())</pre>\n<p>Starts the Streams Application with a shutdown\u00a0hook</p>\n<pre>val kEventStream = new KafkaStreams(builder.build(), config)<br>kEventStream.start()<br>sys.ShutdownHookThread {<br>      kEventStream.close(10, TimeUnit.SECONDS)<br>    }</pre>\n<p>You can send data to the input topic using or programmatically</p>\n<pre>kafka-console-producer --broker-list localhost:9092 --topic inputTopic</pre>\n<p>And can fetch the data from the output topic\u00a0using</p>\n<pre>kafka-console-consumer --bootstrap-server localhost:9092 --topic outputTopic --from-beginning</pre>\n<p>You can add the necessary dependencies in your build file for sbt or pom file for maven. Below is an example for build.sbt.</p>\n<pre>// Kafka<br>libraryDependencies += \"org.apache.kafka\" %% \"kafka-streams-scala\" % \"2.0.0\"<br>libraryDependencies += \"javax.ws.rs\" % \"javax.ws.rs-api\" % \"2.1\" artifacts( Artifact(\"javax.ws.rs-api\", \"jar\", \"jar\")) // this is a workaround. There is an upstream dependency that causes trouble in SBT builds.</pre>\n<p>Let us modify the code a little bit to try out a WordCount example: The code splits the sentences into words and groups by word as a key and the number of occurrences or count as value and is being sent to the output topic by converting the KTable to\u00a0KStream.</p>\n<pre>val textLines: KStream[String, String] = builder.stream[String, String](inputTopic)<br>val wordCounts: KTable[String, Long] = textLines<br>\t\t.flatMapValues(textLine =&gt; textLine.toLowerCase.split(\"\\\\W+\"))<br>\t\t.groupBy((_, word) =&gt; word)<br>\t\t.count()(materializedFromSerde(Serdes.String(),Serdes.Long()))<br>\twordCounts.toStream.to(outputTopic)(producedFromSerde(Serdes.String(),Serdes.Long())</pre>\n<p>With the above process, we can now implement a simple streaming application or a word count application using Kafka Streams in Scala. Check out the <a href=\"https://kafka.apache.org/documentation/streams/\">documentation</a> on their website for more applications!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=5e02ae6f9268\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":["stream-processing","kafka","scala","kafka-streams"]}]}